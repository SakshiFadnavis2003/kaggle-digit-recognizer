{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
      "0      1       0       0       0       0       0       0       0       0   \n",
      "1      0       0       0       0       0       0       0       0       0   \n",
      "2      1       0       0       0       0       0       0       0       0   \n",
      "3      4       0       0       0       0       0       0       0       0   \n",
      "4      0       0       0       0       0       0       0       0       0   \n",
      "\n",
      "   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
      "0       0  ...         0         0         0         0         0         0   \n",
      "1       0  ...         0         0         0         0         0         0   \n",
      "2       0  ...         0         0         0         0         0         0   \n",
      "3       0  ...         0         0         0         0         0         0   \n",
      "4       0  ...         0         0         0         0         0         0   \n",
      "\n",
      "   pixel780  pixel781  pixel782  pixel783  \n",
      "0         0         0         0         0  \n",
      "1         0         0         0         0  \n",
      "2         0         0         0         0  \n",
      "3         0         0         0         0  \n",
      "4         0         0         0         0  \n",
      "\n",
      "[5 rows x 785 columns]\n",
      "(42000, 785)\n",
      "label\n",
      "1    4684\n",
      "7    4401\n",
      "3    4351\n",
      "9    4188\n",
      "2    4177\n",
      "6    4137\n",
      "0    4132\n",
      "4    4072\n",
      "8    4063\n",
      "5    3795\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the dataset\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# Explore the data\n",
    "print(train_df.head())\n",
    "print(train_df.shape)\n",
    "print(train_df['label'].value_counts())\n",
    "\n",
    "# Visualize some examples\n",
    "def plot_examples(data, labels, num_examples=5):\n",
    "    fig, axes = plt.subplots(1, num_examples, figsize=(10, 5))\n",
    "    for i in range(num_examples):\n",
    "        axes[i].imshow(data.iloc[i, 1:].values.reshape(28, 28), cmap='gray')\n",
    "        axes[i].set_title(f'Label: {labels[i]}')\n",
    "        axes[i].axis('off')\n",
    "    plt.show()\n",
    "\n",
    "plot_examples(train_df, train_df['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tusha\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8478 - loss: 0.5203 - val_accuracy: 0.9513 - val_loss: 0.1667\n",
      "Epoch 2/10\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9617 - loss: 0.1295 - val_accuracy: 0.9619 - val_loss: 0.1273\n",
      "Epoch 3/10\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9746 - loss: 0.0839 - val_accuracy: 0.9660 - val_loss: 0.1079\n",
      "Epoch 4/10\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9788 - loss: 0.0639 - val_accuracy: 0.9633 - val_loss: 0.1206\n",
      "Epoch 5/10\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9854 - loss: 0.0484 - val_accuracy: 0.9704 - val_loss: 0.0994\n",
      "Epoch 6/10\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9885 - loss: 0.0354 - val_accuracy: 0.9668 - val_loss: 0.1116\n",
      "Epoch 7/10\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9911 - loss: 0.0288 - val_accuracy: 0.9694 - val_loss: 0.1158\n",
      "Epoch 8/10\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9921 - loss: 0.0238 - val_accuracy: 0.9687 - val_loss: 0.1245\n",
      "Epoch 9/10\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9925 - loss: 0.0212 - val_accuracy: 0.9698 - val_loss: 0.1248\n",
      "Epoch 10/10\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9934 - loss: 0.0199 - val_accuracy: 0.9675 - val_loss: 0.1358\n",
      "\u001b[1m875/875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 640us/step\n",
      "Submission file saved to results/submission.csv\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 744us/step\n",
      "Validation Accuracy: 0.9675\n"
     ]
    }
   ],
   "source": [
    "# digit_recognizer.py\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Create results directory if it does not exist\n",
    "if not os.path.exists('results'):\n",
    "    os.makedirs('results')\n",
    "\n",
    "# Load data\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# Separate features and target\n",
    "X = train_df.drop(columns=['label'])\n",
    "y = train_df['label']\n",
    "\n",
    "# Reshape and normalize pixel values\n",
    "X = X.values.reshape(-1, 28, 28, 1) / 255.0\n",
    "X = X.astype(np.float32)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y = to_categorical(y, 10)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28, 1)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val), verbose=1)\n",
    "\n",
    "# Make predictions\n",
    "test_X = test_df.values.reshape(-1, 28, 28, 1) / 255.0\n",
    "test_X = test_X.astype(np.float32)\n",
    "predictions = model.predict(test_X)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Create a submission file\n",
    "submission_df = pd.DataFrame({\n",
    "    'ImageId': np.arange(1, len(predicted_labels) + 1),\n",
    "    'Label': predicted_labels\n",
    "})\n",
    "\n",
    "# Save the submission file\n",
    "submission_df.to_csv('results/submission.csv', index=False)\n",
    "print(f'Submission file saved to results/submission.csv')\n",
    "\n",
    "# Print accuracy score\n",
    "val_predictions = np.argmax(model.predict(X_val), axis=1)\n",
    "val_labels = np.argmax(y_val, axis=1)\n",
    "print(f'Validation Accuracy: {accuracy_score(val_labels, val_predictions):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tusha\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - accuracy: 0.7267 - loss: 0.8478 - val_accuracy: 0.9705 - val_loss: 0.1050\n",
      "Epoch 2/15\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - accuracy: 0.9516 - loss: 0.1787 - val_accuracy: 0.9810 - val_loss: 0.0687\n",
      "Epoch 3/15\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - accuracy: 0.9659 - loss: 0.1317 - val_accuracy: 0.9813 - val_loss: 0.0708\n",
      "Epoch 4/15\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - accuracy: 0.9693 - loss: 0.1079 - val_accuracy: 0.9830 - val_loss: 0.0587\n",
      "Epoch 5/15\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - accuracy: 0.9768 - loss: 0.0887 - val_accuracy: 0.9849 - val_loss: 0.0514\n",
      "Epoch 6/15\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - accuracy: 0.9801 - loss: 0.0803 - val_accuracy: 0.9837 - val_loss: 0.0615\n",
      "Epoch 7/15\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - accuracy: 0.9819 - loss: 0.0733 - val_accuracy: 0.9851 - val_loss: 0.0617\n",
      "Epoch 8/15\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - accuracy: 0.9820 - loss: 0.0674 - val_accuracy: 0.9840 - val_loss: 0.0608\n",
      "Epoch 9/15\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - accuracy: 0.9853 - loss: 0.0596 - val_accuracy: 0.9863 - val_loss: 0.0558\n",
      "Epoch 10/15\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - accuracy: 0.9850 - loss: 0.0594 - val_accuracy: 0.9864 - val_loss: 0.0522\n",
      "Epoch 11/15\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.9878 - loss: 0.0454 - val_accuracy: 0.9826 - val_loss: 0.0729\n",
      "Epoch 12/15\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - accuracy: 0.9885 - loss: 0.0427 - val_accuracy: 0.9855 - val_loss: 0.0714\n",
      "Epoch 13/15\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.9902 - loss: 0.0392 - val_accuracy: 0.9860 - val_loss: 0.0652\n",
      "Epoch 14/15\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - accuracy: 0.9889 - loss: 0.0450 - val_accuracy: 0.9882 - val_loss: 0.0493\n",
      "Epoch 15/15\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.9901 - loss: 0.0371 - val_accuracy: 0.9885 - val_loss: 0.0503\n",
      "\u001b[1m875/875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step\n",
      "Submission file saved to results/submission.csv\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Validation Accuracy: 0.9885\n"
     ]
    }
   ],
   "source": [
    "# digit_recognizer.py\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout, BatchNormalization\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Create results directory if it does not exist\n",
    "if not os.path.exists('results'):\n",
    "    os.makedirs('results')\n",
    "\n",
    "# Load data\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# Separate features and target\n",
    "X = train_df.drop(columns=['label'])\n",
    "y = train_df['label']\n",
    "\n",
    "# Reshape and normalize pixel values\n",
    "X = X.values.reshape(-1, 28, 28, 1) / 255.0\n",
    "X = X.astype(np.float32)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y = to_categorical(y, 10)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    # Convolutional Layer\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    BatchNormalization(),\n",
    "    \n",
    "    # Convolutional Layer\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    BatchNormalization(),\n",
    "    \n",
    "    # Convolutional Layer\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    BatchNormalization(),\n",
    "    \n",
    "    # Flatten Layer\n",
    "    Flatten(),\n",
    "    \n",
    "    # Fully Connected Layers\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),  # Dropout layer to prevent overfitting\n",
    "    \n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),  # Dropout layer to prevent overfitting\n",
    "    \n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=15, batch_size=32, validation_data=(X_val, y_val), verbose=1)\n",
    "\n",
    "# Make predictions\n",
    "test_X = test_df.values.reshape(-1, 28, 28, 1) / 255.0\n",
    "test_X = test_X.astype(np.float32)\n",
    "predictions = model.predict(test_X)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Create a submission file\n",
    "submission_df = pd.DataFrame({\n",
    "    'ImageId': np.arange(1, len(predicted_labels) + 1),\n",
    "    'Label': predicted_labels\n",
    "})\n",
    "\n",
    "# Save the submission file\n",
    "submission_df.to_csv('results/submission.csv', index=False)\n",
    "print(f'Submission file saved to results/submission.csv')\n",
    "\n",
    "# Print accuracy score\n",
    "val_predictions = np.argmax(model.predict(X_val), axis=1)\n",
    "val_labels = np.argmax(y_val, axis=1)\n",
    "print(f'Validation Accuracy: {accuracy_score(val_labels, val_predictions):.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
